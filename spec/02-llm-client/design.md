# ðŸ¤– LLM Client Design Spec

> **File:** `src/client/index.ts`
> **Model:** Gemini 2.5 Flash (via `@google/generative-ai`)
> **Transport client â†’ server:** stdio (spawned subprocess)

---

## Overview

The LLM Client connects Gemini to the MCP server, creating an interactive
terminal chat where the AI autonomously decides which tools to call.

---

## Diagram 1 â€” Ownership Boundaries

> **Green = you own it** Â· **Blue = Gemini API** Â· **Orange = Google Cloud**

```mermaid
graph TD
    subgraph YOU["âœ… Your Code â€” runs on your machine"]
        CLIENT["MCP Client â€” src/client/index.ts"]
        SERVER["MCP Server â€” src/index.ts"]
        TOOLS["src/tools/ â€” calculator Â· get_weather"]
        RESOURCES["src/resources/ â€” notes"]
        PROMPTS["src/prompts/ â€” templates"]
        ENV[".env â€” API Keys"]
    end

    subgraph GEMINI_API["â˜ï¸ Gemini API â€” Google's infrastructure"]
        LLM["Gemini 2.5 Flash â€” LLM model + reasoning"]
        INFRA["GPU inference servers"]
    end

    subgraph GCP["â˜ï¸ Google Cloud â€” your billing project"]
        QUOTA["API Quota & Rate Limits"]
        BILLING["Billing Account â€” projects/11078345017"]
    end

    CLIENT -->|"HTTPS + API key"| LLM
    CLIENT -->|"stdio subprocess"| SERVER
    SERVER --> TOOLS
    SERVER --> RESOURCES
    SERVER --> PROMPTS
    ENV -.->|"GEMINI_API_KEY"| CLIENT
    LLM --- INFRA
    BILLING --- QUOTA
    CLIENT -->|"governed by"| QUOTA

    classDef green fill:#22c55e,stroke:#16a34a,color:#fff
    classDef blue fill:#3b82f6,stroke:#2563eb,color:#fff
    classDef orange fill:#f97316,stroke:#ea580c,color:#fff
    classDef env fill:#a855f7,stroke:#9333ea,color:#fff

    class CLIENT,SERVER green
    class TOOLS,RESOURCES,PROMPTS green
    class LLM,INFRA blue
    class QUOTA,BILLING orange
    class ENV env
```

---

## Diagram 2 â€” Internal Components

> Color = responsibility: **green** = setup Â· **yellow** = conversion Â· **blue** = AI Â· **purple** = I/O

```mermaid
graph TD
    A["â‘  Validate API Key â€” read GEMINI_API_KEY from .env"]
    B["â‘¡ Spawn MCP Server via StdioClientTransport"]
    C["â‘¢ Discover Tools â€” mcpClient.listTools()"]
    D["â‘£ Convert Schemas â€” MCP Schema â†’ Gemini FunctionDeclaration"]
    E["â‘¤ Init Gemini Model â€” getGenerativeModel(model, tools)"]
    F["â‘¥ Start Chat Session â€” model.startChat()"]
    G["â‘¦ Terminal Input â€” readline.question()"]
    H["â‘§ Agentic Tool Loop â€” while functionCalls() exist"]

    A --> B --> C --> D --> E --> F --> G --> H
    H -->|"next message"| G

    classDef setup fill:#22c55e,stroke:#16a34a,color:#fff
    classDef convert fill:#eab308,stroke:#ca8a04,color:#fff
    classDef ai fill:#3b82f6,stroke:#2563eb,color:#fff
    classDef io fill:#a855f7,stroke:#9333ea,color:#fff
    classDef loop fill:#ef4444,stroke:#dc2626,color:#fff

    class A,B,C setup
    class D convert
    class E,F ai
    class G io
    class H loop
```

---

## Diagram 3 â€” Startup Sequence

> What happens between `npm run client` and the first `You:` prompt.

```mermaid
sequenceDiagram
    actor User as ðŸ‘¤ User
    participant T as Terminal
    participant C as ðŸŸ¢ MCP Client
    participant S as ðŸŸ¢ MCP Server
    participant G as ðŸ”µ Gemini API

    User->>T: npm run client
    T->>C: starts process
    C->>C: read GEMINI_API_KEY from .env
    C->>S: spawn subprocess (tsx src/index.ts)
    S-->>C: âœ… MCP initialize handshake
    C->>S: listTools()
    S-->>C: [calculator, get_weather]
    C->>C: convert schemas â†’ FunctionDeclarations
    C->>G: getGenerativeModel("gemini-2.5-flash", tools)
    G-->>C: model ready
    C->>C: model.startChat()
    C-->>User: ðŸ’¬ "You:" prompt
```

---

## Diagram 4 â€” Live Agentic Loop (per message)

> Every arrow shows an actual function call or API request that happens at runtime.

```mermaid
sequenceDiagram
    actor User as ðŸ‘¤ User
    participant C as ðŸŸ¢ MCP Client
    participant G as ðŸ”µ Gemini 2.5 Flash
    participant S as ðŸŸ¢ MCP Server

    User->>C: "what is 25Ã—4 and weather in Tokyo?"

    rect rgb(219, 234, 254)
        note over C,G: Round 1 â€” Calculator
        C->>G: sendMessage(text + tool definitions)
        activate G
        G-->>C: functionCall: calculator({op:multiply, a:25, b:4})
        deactivate G
        C->>S: callTool("calculator", args)
        activate S
        S-->>C: "25 multiply 4 = 100"
        deactivate S
    end

    rect rgb(220, 252, 231)
        note over C,G: Round 2 â€” Weather
        C->>G: sendMessage(toolResult: "100")
        activate G
        G-->>C: functionCall: get_weather({city:"Tokyo"})
        deactivate G
        C->>S: callTool("get_weather", args)
        activate S
        S-->>C: {temp:"22Â°C", condition:"Partly Cloudy"}
        deactivate S
    end

    rect rgb(254, 243, 199)
        note over C,G: Round 3 â€” Final Answer
        C->>G: sendMessage(toolResult: weather data)
        activate G
        G-->>C: "25Ã—4 is 100. Tokyo is 22Â°C, partly cloudy."
        deactivate G
    end

    C->>User: ðŸ¤– prints final answer
    note over C,User: loops back to "You:" prompt
```

---

## Diagram 5 â€” End-to-End Story (for non-technical readers)

> From typing a command in the terminal to seeing the final answer.
> Each **row** = one phase. Each **phase** flows left â†’ right.

```mermaid
flowchart TD
    A(["â‘  You run: npm run client"])

    subgraph STARTUP["âš¡ Startup  â€”  runs once"]
        direction LR
        B["â‘¡ MCP Client starts"]
        --> C["â‘¢ MCP Server starts in background"]
        --> D["â‘£ Tools discovered: calculator, get_weather"]
        --> E["â‘¤ Tools registered with Gemini"]
    end

    F(["â‘¥ ðŸ’¬ You: prompt appears"])
    G(["â‘¦ You type a question in plain English"])

    subgraph SEND["ðŸ“¡ Send to AI"]
        direction LR
        H["â‘§ Question sent to Gemini over the internet"]
        --> I["â‘¨ Gemini reads question + available tools"]
    end

    DECIDE{"â‘© Does Gemini need a tool?"}

    subgraph TOOL["ðŸ”§ Execute Tool  â€”  repeats per tool"]
        direction LR
        J["â‘ª Gemini names the tool + structured args"]
        --> K["â‘« MCP Client calls tool on MCP Server"]
        --> L["â‘¬ MCP Server runs the action, returns result"]
        --> M["â‘­ Result sent back to Gemini"]
    end

    N["â‘® Gemini writes final answer in plain English"]
    O(["â‘¯ ðŸ¤– Answer printed in terminal"])
    P(["â‘° ðŸ’¬ You: â€” ready for next question"])

    A --> STARTUP --> F --> G --> SEND --> DECIDE
    DECIDE -->|"YES â€” needs a tool"| TOOL
    TOOL -->|"loop back: does it need another?"| DECIDE
    DECIDE -->|"NO â€” has everything"| N
    N --> O --> P
    P -->|"next question"| G

    classDef user fill:#f0fdf4,stroke:#16a34a,color:#166534
    classDef code fill:#22c55e,stroke:#15803d,color:#fff
    classDef ai fill:#3b82f6,stroke:#1d4ed8,color:#fff
    classDef terminal fill:#fef08a,stroke:#ca8a04,color:#713f12
    classDef decide fill:#f97316,stroke:#c2410c,color:#fff

    class A,G,F user
    class B,C,D,E,K,L code
    class H,I,J,M,N ai
    class O,P terminal
    class DECIDE decide
```

> ðŸŸ¢ **Green** = runs on your machine Â· ðŸ”µ **Blue** = internet / Gemini Â· ðŸŸ¡ **Yellow** = what you see in the terminal

---



## Key Design Decisions

| Decision | Rationale |
|----------|-----------|
| **Gemini 2.5 Flash** | Only model available on this billing project |
| **`model.startChat()`** | Keeps full conversation history across turns automatically |
| **Tools at model init** | Gemini requires tool defs at model creation time, not per-request |
| **stdio subprocess** | No separate server startup needed â€” client spawns it automatically |
| **`while` tool loop** | Gemini can chain multiple tool calls before giving a final text answer |

---

## Getting a Gemini API Key

> You need this before you can run the client. Takes ~2 minutes.

1. Go to **[aistudio.google.com/app/apikeys](https://aistudio.google.com/app/apikeys)**
2. Sign in with your Google account if prompted
3. Click **"Create API key"** (top right)
4. Choose **"Default Gemini Project"** (or any existing project)
5. Click **"Create key"** in the dialog
6. **Copy** the full key (starts with `AIza...`)
7. Open `.env` in your project and paste it:
   ```
   GEMINI_API_KEY=AIzaSy...your_key_here
   ```
8. Save the file â€” you're ready to run `npm run client`

> âš ï¸ **If you see quota errors:** Enable billing on your Google Cloud project at
> [console.cloud.google.com/billing](https://console.cloud.google.com/billing).
> This does **not** charge you â€” it just unlocks the free tier quota.

---

## Environment Variables

| Variable | Required | Where |
|----------|----------|-------|
| `GEMINI_API_KEY` | âœ… | `.env` â€” [get one at AI Studio](https://aistudio.google.com/app/apikeys) |

---

## How to Run

```bash
export NVM_DIR="$HOME/.nvm" && . "$NVM_DIR/nvm.sh"
npm run client
```

---

## Extending the Client

To swap in a **different LLM** (e.g. OpenAI), only 3 things change:
1. Replace `@google/generative-ai` with `openai`
2. Rewrite `mcpToolToGeminiFn()` â†’ `mcpToolToOpenAIFn()`
3. Replace `chat.sendMessage()` loop with `chat.completions.create()` + `tool_calls`

**The MCP server stays completely unchanged** â€” that's the power of MCP.
